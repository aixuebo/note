# 背景与总结
## 1.参考"Transformer图1".
## 2.参考"Transformer架构图-图2".

# 一、例子与原理
## 1.例子
问：翻译 我爱你
答：I love you
问：我叫xxx
答: I am called xxx

## 2.疑问
1.为什么直接给我翻译成英语，而不是日语等
2.为什么第二个问题，我并没有提翻译，却依然可以翻译。

## 3.原理
* gpt会根据训练集，预测知道我们下一个字的概率，所以猜测是英文更大的可能。
* 每次输入，都会触发一次调用，把以前的问题也发出去。
比如第二次触发的内容是” 翻译 我爱你 I love you，我叫xxx”。此时配合self，所以会知道继续是翻译的意思。

# 二、经典概率模型 -- 输入 + 词表计算 --> 输出
## 1.训练数据源
我 刚才 跑步，我 现在 吃饭
我 刚才 睡觉，我 现在 吃饭

## 2.词表（Vocabulary） --- 长度L为6的词表
1 我
2 刚才
3 现在
4 吃饭
5 睡觉
6 跑步

## 3.预测概率计算
我 刚才 跑步，我 现在 ___
P(吃饭|跑步) 表示在“跑步”之后，下一个行为是“吃饭”的概率。

输入： “我 刚才 跑步，我 现在”。
循环词表，分别计算跑步和哪个词概率最大，即计算P(？|跑步)。
得到一个结果向量，长度与词典一样。[0, 0, 0, 0.6, 0.3, 0.1]。
所以发现4序号的词是最大的。4对应的是吃饭。

## 4，训练结果是一个矩阵
以上部分，是在训练阶段要做的事情，可以计算好每一个词与其他词之间关系，所以最终训练后得到的是一个矩阵，字典L*L的矩阵。

# 三、Transformer 架构 = 3大部分，输入、编解码、输出
## 1.每一个模块的意义和作用
1.Transformer 接收到“我爱你”这个输入，经过 1- 输入层，2- 编解码层，输出下一个字符 i。
2.此时的输入变为了“我爱你 i”，预测的输出是 love。

输入模块：接收用户输入，然后做内部数据转换，转化后的结果输出给编解码模块。
编解码模块：核心的算法，做概率预测。
输出：根据预测的概率向量，查表，返回下一个字符的输出。

## 2.计算过程
输入矩阵，经过各个算法(embedding,softmax,self-attention,add & norm,feed forward) 输出一个矩阵。
注意： * 每一个算法的结果，都是另外一个算法的输入。
* 每一个算法，都有提前训练好的模型参数。
* 编解码层数为x，表示同样的算法， 要做x次。但是要注意的是每一次矩阵的参数都是不同的。


# 四、Transformer的token词表 -- 是一个矩阵L*M，L表示词表数量，M表示每一个词的Embedding维度
## 1.Transformer中Token比传统的一个词语更细，大概率会精确到字上。比如我叫xxx，会拆分成5个token
## 2.每一个Token并不是用词典序号表示，而是用一个Embedding向量表示。
所以用户的输入“我爱你i”，其实是4个Embedding向量作为输入。

# 五、Transformer的Embedding向量 -- 输出N*M矩阵，N表示输入Token数量，作为算法的输入
## 1.Transformer 架构输入部分第一个流程就是 Embedding
将输入 转换成 输入的词长度N * M的矩阵
## 2.在 GPT-3 里，Embedding 的维度 M = 12288
所以 输入“我爱你i”，输出结果就是4 * 12288的矩阵
## 3.这个矩阵会被传递给编解码模块用作起始输入
一个 Embedding 维度代表一个 Token 的语义属性，维度越高，训练成本就越高，GPT-3 的经验是 M = 12288 维，就足够涌现出类似人类的智能。

# 六、Transformer的Self-Attention算法 -- 编解码模块核心算法 -- 目标就是计算词token的重要性
## 1.自注意力机制（Self-Attention）是编解码模块的第一步，也是最重要的一步，目的是计算输入的每个 Token 在当前句子里的重要性，以便后续算法做预测时更关注那些重要的 Token
因此当我们输入 我叫xxx的时候，他会关注到"翻译"这个重要的词。

## 2.模型需要训练并得到 3 个权重矩阵，分别叫 Wq、Wk、Wv。
都是M*X的矩阵，其中M就是Embedding的长度。即token的长度。

有了这三个权重矩阵，就可以计算好 输入的token的重要性了。

* Wq 是为了生成查询向量，也就是 token1 拿来去向别人查询的向量。
* Wk 是为了生成键向量，也就是 token1 用来回应它人查询的向量。
* Wv 是为了生成值向量，也就是表示 token1 的重要性值的向量。

## 3.计算过程
参考Transformer图1.

# 七、注意事项与总结
## 1.输入参数
* 输入文字
* Token 词表
* 每个 Token 的 Embedding 向量
* Wq、Wk、Wv 权重矩阵
* 其他算法层 Ci x Di 参数矩阵

## 2.变量说明
N 为输入 Token 总数。
M 为 Embedding 维度。
L 为词表总数。

## 3.关键流程是这样的
词的 Token 化 -> Embedding 查询 -> 组成 NxM 输入矩阵 -> Self-Attention 计算 Q，K，V -> Nk 层计算 -> 得到结果向量。

涉及的几个关键参数分别是 Token 词表，

## 4.概念理解
* 模型：就是图里的算法的具体程序实现 + 训练好的模型参数。
* 生成式：一个字一个字预测生成一段文字的方式。
* 参数：就是图里的各个矩阵在模型训练完成后的具体数值。
* 无监督：就是图里应该出现的下一个字正好就是语料里这句话的下一个字，不需要人工标注数据。
* 语义表示：就是图里的 Token 表示为 M 维的 Embedding 向量。
* NLP：就是类似语言翻译这样的自然语言任务。
