# 背景与总结
## 1.注意，这里的关键是数学模型，语言模型是一个由数学公式构建的模型，并不是什么逻辑框架。这个认知非常重要。
通过语料库学习的结果是，每一句话后面要跟的是什么内容，是一个概率，而不是计算机真的理解了人的问题和文字。

比如你问一个问题，你今天吃了吗，他根据历史上语料，回答，吃了，或者没有吃，等着妈妈在做饭呢。
再众多答案中，找到概率最高的返回。

# 一、语言模型
## 1.LLMs，全称是 Large Language Models，即 大语言模型。
ChatGPT 就是一个 LLMs。

## 2.什么是语言模型
就是对人类的语言建立数学模型，
注意，这里的关键是数学模型，语言模型是一个由数学公式构建的模型，并不是什么逻辑框架。这个认知非常重要。


## 3.语言模型当做通信问题处理，这个框架结构对语音和语言处理有着深远的影响，它从根本上使得语音识别有实用的可能

## 4.为何是通讯问题？为何转换成通讯问题后，就能实现语音识别？

根据香农确定的现代通讯原理，所谓的通讯，也被称为信道的编码和解码，信息源先产生原始信息，然后接收方还原一个和原始信息最接近的信息。
比如，你打电话的时候，问对方一句「你吃了晚饭了吗」，在传输前，通讯系统会对这句话进行编码，编成类似「100111101100000…」，
但是传输过程中，一定会有信号损失，接收方收到的编码可能是「1001111011000…」，此时我们就没法解码回原来的句子了。

那如何解决这个问题？

我们可以把与接收到的编码「1001111011000…」类似的句子都罗列出来，可能的情况是：
吃了晚饭了吗
你吃了饭了吗
你吃了晚饭了吗
你吃了晚饭了

然后通讯系统会计算出哪一种的可能性最大，最后把它选出来。只要噪音不大，并且传输信息有冗余，那我们就能复原出原来的信息。

## 5.所以认为让计算机理解人类的语言，不是像教人那样教它语法，而是最好能够让计算机计算出哪一种可能的语句概率最大。
这种计算自然语言每个句子的概率的数学模型，就是语言模型。

# 二、如何计算概率？
## 1.第一阶段是统计语言模型（Statistical Language Model，SLM）-- 其基本思想是基于马尔可夫假设建立词语测模型，根据最近的上下文预测下一个词。
靠输入的上下文进行统计，计算出后续词语的概率，比如「你吃了晚饭了吗」，「你吃了」后面按照概率推算输出"吃饭"。

## 2.第二阶段是神经网络语言模型（Neural Language Model，NLM）
有监督学习，需要人工打标签。

是一个用神经网络来训练模型，学习单词之间的关联性和概率关系。它能够利用大量的数据进行深度学习，从而捕捉到词汇之间更加复杂的关系。
NLM 模型采用的是分层的结构，把输入的文本数据空间投射到高维的语义空间中并进行学习。通过不断地更新神经网络模型参数，NLM 的神经网络逐渐学会了文本数据的语义并能够生成连贯自然、语义准确的文本。

与前面提到的 SLM 相比，由于深度神经网络的学习能力更强，NLM 在学习语言模型时具有更好的泛化能力和适应性。比如能生成更长的文本等。但 NLM 相对来说也比较依赖更大的数据集，并且需要花很多人力在数据标注上。

## 3.第三阶段是预训练语言模型（Pre-trained Language Model，PLM）
Transformer架构就是一种预训练语言模型。
相比于NLM，PLM是一种无监督学习方法。

## 4.第四阶段是大语言模型（Large Language Model）
可以理解成训练数据特别大的 PLM，即无监督学习算法，但需要更大的数据去训练。

# 三、开发大语言模型需要什么？
## 1.数据 -- 需要大量的数据源，因为他要根据数据源，计算每一句话后面要跟的是什么内容，是一种数据概率思维
推荐集中训练的语料库。
* Books：BookCorpus 是之前小语言模型如 GPT-2 常用的数据集，包括超过 11000 本电子书。主要包括小说和传记。
* Gutenberg，它有 70000 本书，包括小说、散文、戏剧等作品，是目前最大的开源书籍语料库之一。
* CommonCrawl：这个是目前最大的开源网络爬虫数据库，不过这个数据包含了大量脏数据，所以目前常用的四个数据库是 C4、CC-Stories、CC-News 和 RealNews。另外还有两个基于 CommonCrawl 提取的新闻语料库 REALNEWS 和 CC-News。
* Reddit Links：简单理解 Reddit 就是外国版本的百度贴吧 + 知乎。目前开源的数据库有 OpenWebText 和 PushShift.io。
* Wikipedia：维基百科是目前大模型广泛使用的语料库。
* Code：一般包含 GitHub 上的开源公共代码库，或者是 StackOverflow 的数据，Google 之前发布过一个叫 BigQuery 的数据集。

实际上，训练大语言模型会混合使用不同的数据，一般不会只使用单一语料库。比如 GPT-3 用的混合数据集就包括 Books、CommonCrowl、Reddit Links 和 Wikipedia。

从数据上看，你需要知道一个事实，中文语料在这上面占比很小。所以一般是中文先翻译成英文，然后再把结果翻译成中文的过程。

## 2.算法
Transformers：这是一个使用 Transformer 架构构建的开源 Python 库。
DeepSpeed：是由微软开发的深度学习优化库。
Megatron-LM：这是由 Nvidia 开发的深度学习库。
JAX：它是由 Google 开发的用于高新能机器学习算法的 Python 库。

## 3.算力

# 四、大语言模型有什么缺点？
## 1.结果高度依赖训练语料​
因为原理本质上是一个概率计算模型，本质上来说它们的核心原理就是「利用已有的信息来预测其他信息」。
如果语料的内容是有偏的（比如带有种族歧视、性别歧视的内容），甚至是错误的，那大语言模型的生成的结果也会是错误的。

所以目前大语言模型可以用在“语料大、且错误率低的领域”，比如像翻译、或者文案生成这种场景

## 2.胡乱说
尤其在语料少的场景下，答案是胡说八道。
